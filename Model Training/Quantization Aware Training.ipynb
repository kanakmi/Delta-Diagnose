{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36e3027e",
   "metadata": {},
   "source": [
    "## Input the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f100b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f3ea45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e7f6561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db606892",
   "metadata": {},
   "source": [
    "## Input the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a4b90d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that would read an image provided the image path, preprocess and return it back\n",
    "\n",
    "def read_and_preprocess(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_COLOR) # reading the image\n",
    "    img = cv2.resize(img, (256, 256)) # resizing it (I just like it to be powers of 2)\n",
    "    img = np.array(img, dtype='float32') # convert its datatype so that it could be normalized\n",
    "    img = img/255 # normalization (now every pixel is in the range of 0 and 1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cccb3bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [] # To store train images\n",
    "y_train = [] # To store train labels\n",
    "\n",
    "# labels -\n",
    "# 0 - Covid\n",
    "# 1 - Viral Pneumonia\n",
    "# 2 - Normal\n",
    "\n",
    "train_path = './dataset/train/' # path containing training image samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d456cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in os.scandir(train_path):\n",
    "    for entry in os.scandir(train_path + folder.name):\n",
    "\n",
    "        X_train.append(read_and_preprocess(train_path + folder.name + '/' + entry.name))\n",
    "        \n",
    "        if folder.name[0]=='C':\n",
    "            y_train.append(0) # Covid\n",
    "        elif folder.name[0]=='V':\n",
    "            y_train.append(1) # Viral Pneumonia\n",
    "        else:\n",
    "            y_train.append(2) # Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6256df4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1955, 256, 256, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_train.shape # We have 1955 training samples in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a01e7ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1955,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d63075d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aug = []\n",
    "y_aug = []\n",
    "\n",
    "for i in range(0, len(y_train)):\n",
    "    X_new = np.fliplr(X_train[i])\n",
    "    X_aug.append(X_new)\n",
    "    y_aug.append(y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d99858ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aug = np.array(X_aug)\n",
    "y_aug = np.array(y_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af9bc1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3910, 256, 256, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.append(X_train, X_aug, axis=0) # appending augmented images to original training samples\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "274443be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3910,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.append(y_train, y_aug, axis=0)\n",
    "y_train.shape\n",
    "# Now we have 3910 samples in total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436ee392",
   "metadata": {},
   "source": [
    "## Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1565f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have splitted our data in a way that - \n",
    "# 1. The samples are shuffled\n",
    "# 2. The ratio of each class is maintained (stratify)\n",
    "# 3. We get same samples every time we split our data (random state)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, shuffle=True, stratify=y_train, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b723ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3323,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will use 3323 images for training the model\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84fbfacc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(587,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will use 587 images for validating the model's performance\n",
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25aa39ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_aug\n",
    "del y_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3484654",
   "metadata": {},
   "source": [
    "## Converting the pretrained model to TFLite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d297492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model (best saved one)\n",
    "with open('./Saved Model/covid_classifier_model.json', 'r') as json_file:\n",
    "    json_savedModel= json_file.read()\n",
    "    \n",
    "# load the model  \n",
    "model = tf.keras.models.model_from_json(json_savedModel)\n",
    "model.load_weights('./Saved Model/covid_classifier_weights.h5')\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=0.0001), metrics= [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fbddf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer (QuantizeLaye (None, 256, 256, 3)       3         \n",
      "_________________________________________________________________\n",
      "quant_conv2d (QuantizeWrappe (None, 255, 255, 32)      483       \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d (Quantiz (None, 63, 63, 32)        1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_1 (QuantizeWrap (None, 63, 63, 64)        18627     \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_1 (Quant (None, 21, 21, 64)        1         \n",
      "_________________________________________________________________\n",
      "quant_dropout (QuantizeWrapp (None, 21, 21, 64)        1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_2 (QuantizeWrap (None, 21, 21, 64)        65731     \n",
      "_________________________________________________________________\n",
      "quant_conv2d_3 (QuantizeWrap (None, 21, 21, 128)       205187    \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_2 (Quant (None, 10, 10, 128)       1         \n",
      "_________________________________________________________________\n",
      "quant_dropout_1 (QuantizeWra (None, 10, 10, 128)       1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_4 (QuantizeWrap (None, 10, 10, 128)       409987    \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_3 (Quant (None, 5, 5, 128)         1         \n",
      "_________________________________________________________________\n",
      "quant_dropout_2 (QuantizeWra (None, 5, 5, 128)         1         \n",
      "_________________________________________________________________\n",
      "quant_flatten (QuantizeWrapp (None, 3200)              1         \n",
      "_________________________________________________________________\n",
      "quant_dense (QuantizeWrapper (None, 512)               1638917   \n",
      "_________________________________________________________________\n",
      "quant_dropout_3 (QuantizeWra (None, 512)               1         \n",
      "_________________________________________________________________\n",
      "quant_dense_1 (QuantizeWrapp (None, 256)               131333    \n",
      "_________________________________________________________________\n",
      "quant_dropout_4 (QuantizeWra (None, 256)               1         \n",
      "_________________________________________________________________\n",
      "quant_dense_2 (QuantizeWrapp (None, 128)               32901     \n",
      "_________________________________________________________________\n",
      "quant_dense_3 (QuantizeWrapp (None, 3)                 392       \n",
      "=================================================================\n",
      "Total params: 2,503,571\n",
      "Trainable params: 2,502,691\n",
      "Non-trainable params: 880\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# q_aware stands for for quantization aware.\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "q_aware_model.compile(optimizer=optimizers.Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b66115f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "104/104 [==============================] - 120s 1s/step - loss: 0.0360 - accuracy: 0.9874 - val_loss: 0.1099 - val_accuracy: 0.9659\n",
      "Epoch 2/5\n",
      "104/104 [==============================] - 122s 1s/step - loss: 0.0425 - accuracy: 0.9874 - val_loss: 0.1003 - val_accuracy: 0.9693\n",
      "Epoch 3/5\n",
      "104/104 [==============================] - 121s 1s/step - loss: 0.0350 - accuracy: 0.9886 - val_loss: 0.0887 - val_accuracy: 0.9642\n",
      "Epoch 4/5\n",
      "104/104 [==============================] - 117s 1s/step - loss: 0.0342 - accuracy: 0.9883 - val_loss: 0.1055 - val_accuracy: 0.9642\n",
      "Epoch 5/5\n",
      "104/104 [==============================] - 116s 1s/step - loss: 0.0337 - accuracy: 0.9871 - val_loss: 0.0874 - val_accuracy: 0.9710\n"
     ]
    }
   ],
   "source": [
    "history = q_aware_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e086f7",
   "metadata": {},
   "source": [
    "## Loading the Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6c0b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [] # To store test images\n",
    "y_test = [] # To store test labels\n",
    "\n",
    "test_path = './dataset/test/'\n",
    "\n",
    "for folder in os.scandir(test_path):\n",
    "    for entry in os.scandir(test_path + folder.name):\n",
    "\n",
    "        X_test.append(read_and_preprocess(test_path + folder.name + '/' + entry.name))\n",
    "        \n",
    "        if folder.name[0]=='C':\n",
    "            y_test.append(0)\n",
    "        elif folder.name[0]=='V':\n",
    "            y_test.append(1)\n",
    "        else:\n",
    "            y_test.append(2)\n",
    "            \n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e11d3d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185, 256, 256, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape # We have 185 images for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54930b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions\n",
    "predictions = q_aware_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32f543ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the predicted class from the model prediction\n",
    "predict = []\n",
    "\n",
    "for i in predictions:\n",
    "  predict.append(np.argmax(i))\n",
    "\n",
    "predict = np.asarray(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d51042ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9891891891891892"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the accuracy of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, predict)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e1a060b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, predict)\n",
    "plt.figure(figsize = (7,7))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues')\n",
    "\n",
    "# The model misclassified one Covid case as Normal and one Normal case as Viral Pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "237f086a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        58\n",
      "           1       0.98      1.00      0.99        63\n",
      "           2       0.98      0.98      0.98        64\n",
      "\n",
      "    accuracy                           0.99       185\n",
      "   macro avg       0.99      0.99      0.99       185\n",
      "weighted avg       0.99      0.99      0.99       185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test, predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fffcd649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, dropout_layer_call_fn while saving (showing 5 of 75). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\kanak\\AppData\\Local\\Temp\\tmp5lkdlpmq\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\kanak\\AppData\\Local\\Temp\\tmp5lkdlpmq\\assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_qaware_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc948a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size reduced from 30.1 MB to  2.41  MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Model size reduced from 30.1 MB to \", np.around(len(tflite_qaware_model)/(1024*1024),2), \" MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "284f2807",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"covid_classifier.tflite\", 'wb') as f:\n",
    "    f.write(tflite_qaware_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
